{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "h_BDjNUCr8wM"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import plotly\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import gaussian_kde\n",
        "import matplotlib\n",
        "from random import sample\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Heatmap**"
      ],
      "metadata": {
        "id": "iCHYjvIasHiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV data into DataFrames\n",
        "df_avian = pd.read_csv(\"gs_gd_avian.csv\")\n",
        "df_swine = pd.read_csv(\"gs_gd_swine.csv\")\n",
        "df_human = pd.read_csv(\"gs_gd_human.csv\")\n",
        "\n",
        "freq_list = [[\"n = 33\", \"n = 4860\", \"n = 3982\", \"n = 8668\"],\n",
        "             [\"n = 0\", \"n = 34\", \"n = 19\", \"n = 0\"],\n",
        "             [\"n = 19\", \"n = 542\", \"n = 207\", \"n = 26\"]]\n",
        "\n",
        "title = [\"Avian\", \"Swine\", \"Human\"]\n",
        "for df in [df_avian, df_swine, df_human]:\n",
        "    df.set_index(\"Proteins\", inplace=True)\n",
        "    df.round(2)\n",
        "    df.dropna(inplace=True)\n",
        "\n",
        "# Function to create mask and annotation DataFrame\n",
        "def create_mask_annot(df):\n",
        "    mask = df >= 0.01\n",
        "    annot_df = df.mask(~mask, \"\")\n",
        "    return annot_df\n",
        "\n",
        "annot_df_avian = create_mask_annot(df_avian)\n",
        "annot_df_swine = create_mask_annot(df_swine)\n",
        "annot_df_human = create_mask_annot(df_human)\n"
      ],
      "metadata": {
        "id": "ltuAIFBtsXIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(font_scale=1.2)\n",
        "\n",
        "# Create the figure and axes\n",
        "fig, axs = plt.subplots(3, 1, figsize=(8, 8))\n",
        "\n",
        "# Plot the heatmaps\n",
        "for i, (ax, df, annot_df) in enumerate(zip(axs, [df_avian, df_swine, df_human], [annot_df_avian, annot_df_swine, annot_df_human])):\n",
        "    heatmap = sns.heatmap(df, annot=annot_df, fmt=\"\",\n",
        "                          cmap=sns.cubehelix_palette(dark=.25, light=.85, as_cmap=True),\n",
        "                          linewidth=0.75, cbar=True, xticklabels=True, yticklabels=True, ax=ax, cbar_kws={'pad': 0.18})\n",
        "\n",
        "    heatmap.set_yticklabels(heatmap.get_yticklabels(), rotation=0)\n",
        "\n",
        "    # Add x tick labels only to the first plot\n",
        "    if i == 0:\n",
        "        heatmap.tick_params(axis='x', top=True, labeltop=True, bottom=False, labelbottom=False)\n",
        "    else:\n",
        "        heatmap.tick_params(axis='x', top=False, labeltop=False, bottom=False, labelbottom=False)\n",
        "\n",
        "    heatmap.set_xticklabels(heatmap.get_xticklabels(), rotation=90)\n",
        "    ax.set_ylabel(\" \")\n",
        "    # Create a twin y-axis to add labels on the right\n",
        "    ax2 = ax.twinx()\n",
        "    ax2.set_ylim(ax.get_ylim())\n",
        "    ax2.set_yticks(ax.get_yticks())\n",
        "    ax2.set_yticklabels(freq_list[i], rotation=0)\n",
        "\n",
        "    # Hide the right y-axis ticks and label\n",
        "    ax2.tick_params(axis='y', which='both', length=0)\n",
        "    ax2.set_ylabel(\" \")\n",
        "    ax2.grid(False)\n",
        "\n",
        "    # Make sure both axes share the same position\n",
        "    pos1 = ax.get_position()\n",
        "    ax2.set_position(pos1)\n",
        "\n",
        "    for _, spine in heatmap.spines.items():\n",
        "        spine.set_visible(True)\n",
        "        spine.set_color('black')\n",
        "        spine.set_linewidth(1)\n",
        "\n",
        "# Add titles outside the loop\n",
        "for i, ax in enumerate(axs):\n",
        "    ax.set_title(title[i], fontweight=\"bold\")\n",
        "\n",
        "# Adjust the vertical spacing between subplots\n",
        "plt.subplots_adjust(hspace=0.4)  # Change 0.4 to your desired spacing\n",
        "\n",
        "plt.ylabel(\" \", rotation=0, labelpad=20)\n",
        "\n",
        "# Adjust layout to avoid clipping of tick labels\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save the figure\n",
        "plt.savefig('multiple_heatmaps.tiff', dpi = 600, bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vZnC1JStsG_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NA Stalk length analysis**"
      ],
      "metadata": {
        "id": "XkGf7MX_tN7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_msa(input_file,n):\n",
        "\n",
        "  lines_to_concatenate = n\n",
        "  with open(input_file, \"r\") as file:\n",
        "    lines = file.readlines()\n",
        "  # Initialize a list to store lines to be appended\n",
        "  appended_lines = []\n",
        "  # Iterate through the lines and concatenate lines from each block\n",
        "  for i in range(lines_to_concatenate):\n",
        "    for j in range(i, len(lines), lines_to_concatenate):\n",
        "        appended_lines.append(lines[j].strip())\n",
        "  # Write the extracted lines into the output file\n",
        "  with open(\"output.txt\", \"w\") as file:\n",
        "    file.write(\"\\n\".join(appended_lines))\n",
        "\n",
        "\n",
        "  lines = []\n",
        "  with open(\"output.txt\",\"r\") as file:\n",
        "    lines.append(file.readlines())\n",
        "  # Initialize variables to store the concatenated strings\n",
        "  concatenated_strings = []\n",
        "  current_string = \"\"\n",
        "  # Iterate through the list of strings\n",
        "  for line in lines[0]:\n",
        "    if '\\t' not in line:\n",
        "      # If there is no '\\t' in the line, add it to the current string\n",
        "      current_string += line.strip()\n",
        "    else:\n",
        "      # If a '\\t' is found, add it to the current string and append it to the concatenated_strings list\n",
        "      current_string += line.strip()\n",
        "      concatenated_strings.append(current_string)\n",
        "      current_string = \"\"  # Reset the current string\n",
        "\n",
        "\n",
        "  return concatenated_strings"
      ],
      "metadata": {
        "id": "5ocUKtLKtNgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stalk(input_string,stak_begin_regex,stalk_end_regex, A, A_no, B, B_no):\n",
        "\n",
        "  match = re.search(stak_begin_regex, input_string)\n",
        "  if match:\n",
        "    matched_text = match.group()\n",
        "    start_index = match.start()\n",
        "    end_index = match.end()\n",
        "    # Find the index of the second occurrence of \"I\" within the matched text\n",
        "    second_i_index = matched_text.find(A, matched_text.find(A) +A_no )\n",
        "  else:\n",
        "    print(\"Stalk beginning not found.\")\n",
        "\n",
        "  start_index_stalk = start_index+second_i_index\n",
        "\n",
        "\n",
        "  # Search for the pattern in the input string, case-insensitive\n",
        "  match = re.search(stalk_end_regex, input_string, re.IGNORECASE)\n",
        "\n",
        "  if match:\n",
        "    matched_text = match.group()\n",
        "    start_index_ = match.start()\n",
        "    end_index_ = match.end()\n",
        "\n",
        "    # Find the index of the second occurrence of \"x\" within the matched text\n",
        "    second_i_index_ = matched_text.find(B, matched_text.find(B) +B_no )\n",
        "  else:\n",
        "    print(\"Stalk end not found.\")\n",
        "\n",
        "  end_index_stalk = start_index_ + second_i_index_\n",
        "\n",
        "  return [start_index_stalk,end_index_stalk]\n"
      ],
      "metadata": {
        "id": "QIZ_X5M2tlud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stalk_length_year(input_file_msa, input_msa_date, n, stak_begin_regex ,stalk_end_regex, A, A_no, B, B_no ):\n",
        "\n",
        "  # Open the input file for reading and the output file for writing\n",
        "  with open(input_file_msa, 'r') as infile, open(\"output_\"+input_file_msa, 'w') as outfile:\n",
        "    # Iterate through each line in the input file\n",
        "    for line in infile:\n",
        "        # Check if the line is not blank (contains at least one character)\n",
        "        if line.strip():\n",
        "            # If the line is not blank, write it to the output file\n",
        "            outfile.write(line)\n",
        "\n",
        "  msa = clean_msa(\"output_\"+input_file_msa,n)\n",
        "  stalk_cs = get_stalk(msa[0],stak_begin_regex ,stalk_end_regex,A, A_no, B, B_no)\n",
        "\n",
        "  # for every sequence in msa except cs isolate the stalk\n",
        "  stalk_list = []\n",
        "  for i in range(1,len(msa)):\n",
        "    stalk_list.append(msa[i][stalk_cs[0]:stalk_cs[1]])\n",
        "  # for every stalk get length without counting -\n",
        "  stalk_length= []\n",
        "\n",
        "  for input_string in stalk_list:\n",
        "    # Remove hyphens from the input string\n",
        "    input_string_without_hyphens = input_string.replace(\"-\", \"\")\n",
        "\n",
        "    # Calculate the length of the modified string and append it to the lengths list\n",
        "    length_without_hyphens = len(input_string_without_hyphens)\n",
        "    stalk_length.append(length_without_hyphens)\n",
        "\n",
        "  lines = []\n",
        "  with open(input_msa_date,\"r\") as f:\n",
        "    lines.append(f.readlines())\n",
        "\n",
        "  regex_year = '\\|\\d{4}-\\d{2}-\\d{2}\\|'\n",
        "  years = []\n",
        "  for i in range(0,len(lines[0])):\n",
        "    input_string = lines[0][i]\n",
        "    # Search for the pattern in the input string, case-insensitive\n",
        "    match = re.search(regex_year, input_string)\n",
        "    if match:\n",
        "      matched_text = match.group()\n",
        "\n",
        "      years.append(int(matched_text.split('-')[0][1:5]))\n",
        "\n",
        "  lines = []\n",
        "  with open(input_msa_date,\"r\") as f:\n",
        "    lines.append(f.readlines())\n",
        "\n",
        "  regex_epi =  r'\\|EPI_ISL_\\d+\\|'\n",
        "  epi = []\n",
        "  for i in range(0,len(lines[0])):\n",
        "    input_string = lines[0][i]\n",
        "  # Search for the pattern in the input string, case-insensitive\n",
        "    match = re.search(regex_epi, input_string)\n",
        "    if match:\n",
        "      matched_text = match.group()\n",
        "      epi.append(matched_text)\n",
        "\n",
        "  ha_na_regex = '\\|A_/_H\\d{1,2}N\\d{1,2}\\|'\n",
        "  HA = []\n",
        "  for i in range(0,len(lines[0])):\n",
        "    input_string = lines[0][i]\n",
        "  # Search for the pattern in the input string, case-insensitive\n",
        "    match = re.search(ha_na_regex, input_string)\n",
        "    if match:\n",
        "      matched_text = match.group()\n",
        "      HA.append(matched_text)\n",
        "\n",
        "  return years, msa[1:], stalk_list,stalk_length, epi, HA"
      ],
      "metadata": {
        "id": "jNr86w1Ktn6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Avian stalk length\n",
        "years_batch1,full_sequences_1, stalk_list_1, stalk_length_batch1, epi_batch1, _ =  get_stalk_length_year(\"N1-msa-batch1.txt\", \"N1-msa-date-batch1.txt\", 4000, r'G[-]*N[-]*I[-]*I[-]*S[-]*x[-]*x', r'V[-]*x[-]*x[-]*x[-]*T[-]*L', \"I\", 1, \"x\", 1)\n",
        "years_batch2,full_sequences_2, stalk_list_2, stalk_length_batch2, epi_batch2, _  =  get_stalk_length_year(\"N1-msa-batch2.txt\", \"N1-msa-date-batch2.txt\", 4000, r'G[-]*N[-]*I[-]*I[-]*S[-]*x[-]*x', r'V[-]*x[-]*x[-]*x[-]*T[-]*L', \"I\", 1, \"x\", 1)\n",
        "years_batch3,full_sequences_3, stalk_list_3, stalk_length_batch3, epi_batch3, _  =  get_stalk_length_year(\"N1-msa-batch3.txt\", \"N1-msa-date-batch3.txt\", 4000, r'G[-]*N[-]*I[-]*I[-]*S[-]*x[-]*x', r'V[-]*x[-]*x[-]*x[-]*T[-]*L', \"I\", 1, \"x\", 1)\n",
        "years_batch4,full_sequences_4, stalk_list_4, stalk_length_batch4, epi_batch4, _  =  get_stalk_length_year(\"N1-msa-batch4.txt\", \"N1-msa-date-batch4.txt\", 2322, r'G[-]*N[-]*I[-]*I[-]*S[-]*x[-]*x', r'V[-]*x[-]*x[-]*x[-]*T[-]*L', \"I\", 1, \"x\", 1)\n",
        "\n",
        "years = years_batch1 + years_batch2 + years_batch3 + years_batch4\n",
        "full_sequences = full_sequences_1+full_sequences_2+full_sequences_3 + full_sequences_4\n",
        "stalk_list = stalk_list_1 + stalk_list_2 + stalk_list_3 + stalk_list_4\n",
        "stalk_length = stalk_length_batch1 + stalk_length_batch2+ stalk_length_batch3 + stalk_length_batch4\n",
        "epi = epi_batch1 + epi_batch2 + epi_batch3 + epi_batch4\n",
        "\n",
        "df_Avian_N1 = pd.DataFrame({\"Years\":years, \"full_sequence\":full_sequences, \"Stalk_seq\": stalk_list, \"Stalk_length\":stalk_length, \"epi\":epi})"
      ],
      "metadata": {
        "id": "lDXQwSAY0Tl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Avian_N1 = df_Avian_N1[(df_Avian_N1[\"Years\"] > 1989) & (df_Avian_N1[\"Stalk_length\"] > 0)]\n",
        "\n",
        "decades = []\n",
        "\n",
        "for year in df_Avian_N1[\"Years\"]:\n",
        "    if 1990 <= year <= 1999:\n",
        "        decades.append(\"1990-99\")\n",
        "    elif 2000 <= year <= 2009:\n",
        "        decades.append(\"2000-09\")\n",
        "    elif 2010 <= year <= 2019:\n",
        "        decades.append(\"2010-19\")\n",
        "    elif 2020 <= year <= 2023:\n",
        "        decades.append(\"2020-23\")\n",
        "\n",
        "df_Avian_N1[\"Decade\"] = decades\n",
        "df_Avian_N1_90_99 = df_Avian_N1[df_Avian_N1[\"Decade\"] == \"1990-99\"]\n",
        "df_Avian_N1_00_09 = df_Avian_N1[df_Avian_N1[\"Decade\"] == \"2000-09\"]\n",
        "df_Avian_N1_10_19 = df_Avian_N1[df_Avian_N1[\"Decade\"] == \"2010-19\"]\n",
        "df_Avian_N1_20_23 = df_Avian_N1[df_Avian_N1[\"Decade\"] == \"2020-23\"]"
      ],
      "metadata": {
        "id": "dS_IDLTS0XQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set(font_scale = 2)\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "sns.kdeplot(df_Avian_N1_90_99[\"Stalk_length\"], fill=True, palette=\"ch:.35\",color = \"blue\", label=\"1990-99\",  alpha = 0.5)\n",
        "sns.kdeplot(df_Avian_N1_00_09[\"Stalk_length\"], fill=True, palette=\"ch:.35\",color = \"orange\", label=\"2000-09\",  alpha = 0.5)\n",
        "sns.kdeplot(df_Avian_N1_10_19[\"Stalk_length\"], fill=True, palette=\"ch:.35\",color = \"green\", label=\"2010-19\",  alpha = 0.5)\n",
        "sns.kdeplot(df_Avian_N1_20_23[\"Stalk_length\"], fill=True, palette=\"ch:.35\",color = \"red\", label=\"2020-23\",  alpha = 0.5)\n",
        "\n",
        "plt.legend()\n",
        "ax.legend(title = \"Years\", loc='upper center', bbox_to_anchor=(0.16, 1.03), ncol=1)\n",
        "plt.xlabel('Stalk length', fontsize = 30, labelpad=10)\n",
        "plt.ylabel('Density', fontsize = 30, labelpad=10)\n",
        "\n",
        "\n",
        "\n",
        "plt.xlim(10,70)\n",
        "plt.ylim(0,0.6)\n",
        "plt.yticks(rotation=0, fontsize=28)\n",
        "plt.xticks(rotation=0, fontsize=28)\n",
        "\n",
        "\n",
        "plt.savefig('Avian_N1_stalk_length.tiff', dpi = 600, bbox_inches='tight')"
      ],
      "metadata": {
        "id": "uzSSnRyj0vZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HA glycosylation**"
      ],
      "metadata": {
        "id": "4rL8R4md1QBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_HA_df(fasta_file):\n",
        "  # Define the regex patterns to search for within the header lines\n",
        "  pattern_epid = r'\\|EPI_ISL_\\d+\\|'\n",
        "  pattern_date = r'\\d{4}-\\d{2}-\\d{2}'\n",
        "\n",
        "  # Initialize lists to store identifiers, dates, and sequences\n",
        "  identifiers = []\n",
        "  dates = []\n",
        "  sequences = []\n",
        "\n",
        "  # Open the FASTA file for reading\n",
        "  with open(fasta_file, \"r\") as fasta_file:\n",
        "    current_header = \"\"\n",
        "    current_sequence = \"\"\n",
        "    for line in fasta_file:\n",
        "        line = line.strip()\n",
        "        # Check if the line is a header line\n",
        "        if line.startswith('>'):\n",
        "          # If we already have a header and sequence, process them\n",
        "            if current_header:\n",
        "              epi_match = re.search(pattern_epid, current_header)\n",
        "              date_match = re.search(pattern_date, current_header)\n",
        "\n",
        "              if epi_match and date_match:\n",
        "                identifiers.append(epi_match.group(0))\n",
        "                dates.append(date_match.group(0))\n",
        "                sequences.append(current_sequence)\n",
        "\n",
        "              current_sequence = \"\"\n",
        "\n",
        "            current_header = line\n",
        "        else:\n",
        "            current_sequence += line\n",
        "\n",
        "    # Process the last sequence\n",
        "    if current_header:\n",
        "        epi_match = re.search(pattern_epid, current_header)\n",
        "        date_match = re.search(pattern_date, current_header)\n",
        "\n",
        "        if epi_match and date_match:\n",
        "            identifiers.append(epi_match.group(0))\n",
        "            dates.append(date_match.group(0))\n",
        "            sequences.append(current_sequence)\n",
        "  # Create a DataFrame\n",
        "  data = {'EPI_ID': identifiers, 'Date': dates, 'H5_Sequence': sequences}\n",
        "  df = pd.DataFrame(data)\n",
        "\n",
        "  # Identify the putative glycosylation sites\n",
        "  gls = r'[Nn][A-Za-z][SsTt]'\n",
        "  gls_no = []\n",
        "\n",
        "  for i in df[\"H5_Sequence\"]:\n",
        "    matches = re.findall(gls,i)\n",
        "    match_count = len(matches)\n",
        "    df.loc[df[\"H5_Sequence\"] == i, \"#GLS\"] = match_count\n",
        "    gls_no.append(match_count)\n",
        "\n",
        "  df[\"#GLS\"] = gls_no\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "7IzHTJZE2aDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Avian stalk length - clade 2.3.4.4b\n",
        "years_batch1,full_sequences_1, stalk_list_1, stalk_length_batch1, epi_batch1,_ =  get_stalk_length_year(\"N1-2.3.4.4b-batch1-msa.txt\", \"N1-2.3.4.4b-batch1-msa-date.txt\",4000, r'G[-]*N[-]*I[-]*I[-]*S[-]*x[-]*x', r'V[-]*x[-]*x[-]*x[-]*T[-]*L', \"I\", 1, \"x\", 1)\n",
        "years_batch2,full_sequences_2, stalk_list_2, stalk_length_batch2, epi_batch2,_ =  get_stalk_length_year(\"N1-2.3.4.4b-batch2-msa.txt\", \"N1-2.3.4.4b-batch2-msa-date.txt\", 2643, r'G[-]*N[-]*I[-]*I[-]*S[-]*x[-]*x', r'V[-]*x[-]*x[-]*x[-]*T[-]*L', \"I\", 1, \"x\", 1)\n",
        "\n",
        "\n",
        "years = years_batch1 + years_batch2\n",
        "full_sequences = full_sequences_1 + full_sequences_2\n",
        "stalk_list = stalk_list_1 + stalk_list_2\n",
        "stalk_length = stalk_length_batch1  + stalk_length_batch2\n",
        "epi_id = epi_batch1 + epi_batch2\n",
        "\n",
        "\n",
        "df_Avian_N1_4b = pd.DataFrame({\"Years\":years, \"full_sequence\":full_sequences, \"Stalk_seq\": stalk_list, \"NA_Stalk_length\":stalk_length, \"Epi_id\": epi_id})\n",
        "df_Avian_N1_4b_ls = df_Avian_N1_4b[40 < df_Avian_N1_4b[\"NA_Stalk_length\"] < 70]\n",
        "df_Avian_N1_4b_ss = df_Avian_N1_4b[20 < df_Avian_N1_4b[\"NA_Stalk_length\"] < 41]\n",
        "\n",
        "# Get EPI ID and then sort the H5\n",
        "epi_ls = df_Avian_N1_4b_ls[\"Epi_id\"].to_list()\n",
        "epi_ss = df_Avian_N1_4b_ss[\"Epi_id\"].to_list()"
      ],
      "metadata": {
        "id": "nrFqZS-G2gra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Avian H5 dataframe - clade 2.3.4.4b H5N1\n",
        "df_H5_4b = get_HA_df(\"H5N1-H5-2.3.4.4b.fasta\")\n",
        "df_H5_4b_ls = df_H5_4b[df_H5_4b[\"EPI_ID\"].isin(epi_ls)]\n",
        "df_H5_4b_ss = df_H5_4b[df_H5_4b[\"EPI_ID\"].isin(epi_ss)]"
      ],
      "metadata": {
        "id": "IidgBTeg3WnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Avian stalk length - clade 2.3.2.1a and 1c\n",
        "years,full_sequences, stalk_list, stalk_length, epi_id,_ =  get_stalk_length_year(\"H5N1-N1-2.3.2.1a,1c-msa.txt\", \"H5N1-N1-2.3.2.1a,1c-msa-date.txt\",343, r'G[-]*N[-]*I[-]*I[-]*S[-]*x[-]*x', r'V[-]*x[-]*x[-]*x[-]*T[-]*L', \"I\", 1, \"x\", 1)\n",
        "\n",
        "df_Avian_N1_1a_1c = pd.DataFrame({\"Years\":years, \"full_sequence\":full_sequences, \"Stalk_seq\": stalk_list, \"NA_Stalk_length\":stalk_length, \"Epi_id\": epi_id})\n",
        "df_Avian_N1_1a_1c_ls = df_Avian_N1_1a_1c[40 < df_Avian_N1_1a_1c[\"NA_Stalk_length\"] < 70]\n",
        "df_Avian_N1_1a_1c_ss = df_Avian_N1_1a_1c[20 < df_Avian_N1_1a_1c[\"NA_Stalk_length\"] < 41]\n",
        "\n",
        "# GET EPI ID and then sort the H5\n",
        "epi_ls = df_Avian_N1_1a_1c_ls[\"Epi_id\"].to_list()\n",
        "epi_ss = df_Avian_N1_1a_1c_ss[\"Epi_id\"].to_list()"
      ],
      "metadata": {
        "id": "1sB7O4C84eWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Avian H5 dataframe - clade 2.3.2.1a and 1c H5N1\n",
        "df_H5_1a_1c = get_HA_df(\"H5N1-H5-2.3.2.1a, 1c.fasta\")\n",
        "\n",
        "df_H5_1a_1c_ls = df_H5_1a_1c[df_H5_1a_1c[\"EPI_ID\"].isin(epi_ls)]\n",
        "df_H5_1ac_ss = df_H5_1a_1c[df_H5_1a_1c[\"EPI_ID\"].isin(epi_ss)]"
      ],
      "metadata": {
        "id": "onI0sOHI430X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize = (8,6))\n",
        "sns.set_style(\"whitegrid\")\n",
        "sns.kdeplot(df_H5_4b_ls[\"#GLS\"], fill=True, label = \"2.3.4.4b H5N1-ls\", color = \"green\", alpha = 0.6)\n",
        "sns.kdeplot(df_H5_4b_ss[\"#GLS\"], fill=True, label = \"2.3.4.4b H5N1-ss\", color = \"orange\", alpha = 0.8)\n",
        "sns.kdeplot(df_H5_1a_1c_ls[\"#GLS\"], fill=True, label = \"2.3.2.1a H5N1-ss\", color = \"red\", alpha = 0.9)\n",
        "sns.kdeplot(df_H5_1ac_ss[\"#GLS\"], fill=True, label = \"2.3.2.1c H5N1-ss\", color = \"violet\", alpha = 0.7)\n",
        "\n",
        "\n",
        "plt.xlim(4,12)\n",
        "ax.set_xticks(np.arange(4, 13, 1))\n",
        "\n",
        "plt.ylim(0,6)\n",
        "\n",
        "# Move the legend below the plot in one row\n",
        "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.30), ncol=2)\n",
        "\n",
        "# Set labels and title\n",
        "plt.xlabel('No.of glycosylation sites')\n",
        "plt.ylabel('Density')\n",
        "#plt.title('H5N1 GLS coverage distribution 2018-2023')\n",
        "\n",
        "matplotlib.rcParams.update({'font.size': 18})\n",
        "\n",
        "# Save the plot\n",
        "plt.savefig('H5N1_gls_18_23.tiff',  dpi=600,bbox_inches='tight')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2ePGEbNR5L9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bubble plot**"
      ],
      "metadata": {
        "id": "0OW8bxZF5kNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"H5_2018_23.csv\")\n",
        "\n",
        "fig = px.scatter(df, x=\"#GLS\", y=\"NA type\",\n",
        "                 size=\"Frequency\", color=\"NA_Stalk_length\", size_max=80, color_continuous_scale=\"Plasma\")\n",
        "\n",
        "fig.update_layout(\n",
        "    font=dict(\n",
        "        family=\"Arial\",\n",
        "        size=20,  # Set the font size here\n",
        "        #color=\"RebeccaPurple\"\n",
        "    )\n",
        ")\n",
        "fig.update_layout(width=700, height=500)\n",
        "fig.update_xaxes(range=[4, 10])\n",
        "fig.update_xaxes(title=\"\")\n",
        "fig.update_yaxes(title=\"\")\n",
        "\n",
        "fig.update_traces(marker=dict(colorscale='Purpor'))\n",
        "\n",
        "# Hide the color bar\n",
        "fig.update_layout(coloraxis_showscale=False)\n",
        "\n",
        "fig.write_image(\"H5_bubble_2018_23.pdf\")\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "xsuQnapg5mSd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}